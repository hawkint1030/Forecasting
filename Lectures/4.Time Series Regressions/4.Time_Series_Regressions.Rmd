---
title: "Applied Economic Forecasting"
subtitle: "4. Time Series Regressions"
date: "Spring 2020"
autosize: true
fontsize: 11pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: madrid
    colortheme: "beaver"
    fonttheme: "serif"
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(fpp2)
options(width=50)
```

# The linear model with time series

## Multiple regression and forecasting
\fontsize{13}{15}\selectfont
\begin{block}{}\vspace*{-0.3cm}
$$
  y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t} + \cdots + \beta_kx_{k,t} + \varepsilon_t.
$$
\end{block}

* $y_t$ is the variable we want to predict: the ``response'' variable
* Each $x_{j,t}$ is numerical and is called a ``predictor''.
 They are usually assumed to be known for all past and future times.

* The coefficients $\beta_1,\dots,\beta_k$ measure the effect of each
predictor after taking account of the effect of all other predictors
in the model.

  * That is, the coefficients measure the **marginal effects**.

* $\varepsilon_t$ is a white noise error term

## Digression: Pipe Operators in `R`
\scriptsize
In this chapter, we will use our pipe operator (` %>% `) quite a bit. Recall that this allows us to pass a variable or argument to (an)other function(s) or operation(s).

\structure{Example}

We would like to execute the following tasks:

```{r, tidy=TRUE, tidy.opts=c(width.cutoff=70),fig.height=1.75}
# 1.  Store a set of values in `x`
x <- rnorm(120,0,3) + 1:120 + 20*sin(2*pi*(1:120)/12)
# 2. Convert to ts object of frequency = 12, ending 2019,12, 
tsx <- ts(x, end = c(2019,12),frequency = 12)
# 3. Using a horizon of 24 months, estimate a seasonal naÃ¯ve model
f.tsx <- snaive(tsx, h= 24)
# 4. Produce a plot of the actual and forecasted values
autoplot(tsx) + autolayer(f.tsx) + labs(y =" ", x="") +ggtitle("My Graph") 
  
```


## Digression: Pipe Operators in `R` {.plain}

\scriptsize
With the help of `%<%`, you can rewrite the above lines of code as:
```{r pipe1, fig.height=3, tidy=TRUE}
(rnorm(120,0,3) + 1:120 + 20*sin(2*pi*(1:120)/12)) %>% ts(end = c(2019,12), frequency = 12) %>%  snaive(h= 24) %>% autoplot() + labs(y =" ", x = "") + ggtitle("My Graph")
```

Note: 
It isn't a formal requirement to add the parentheses after "autoplot" but it enhances the readability of the code.

**END OF DIGRESSION**

## Example: US consumption expenditure

\scriptsize
```{r ConsInc, fig.height=3.5, tidy = TRUE}
autoplot(uschange[,c("Consumption","Income")]) + ylab("% change") + xlab("Year")
```

## Example: US consumption expenditure

\scriptsize
```{r, tidy=TRUE}
tslm(Consumption ~ Income, data=uschange) %>% summary
```

## Example: US consumption expenditure
\scriptsize
```{r ConsInc2,tidy=TRUE}
fit.cons <- tslm(Consumption ~ Income, data=uschange)
uschange %>%  as.data.frame %>%  ggplot(aes(x=Income, y=Consumption)) +
    labs(y="Consumption (quarterly % change)", x = "Income (quarterly % change)") +
    geom_point() + geom_smooth(method="lm", se=FALSE)
```


## Example: US consumption expenditure
\scriptsize
```{r MultiPredictors, tidy= TRUE}
autoplot(uschange[,3:5], facets = TRUE, colour=TRUE) + labs(y = "", x = "Year") +  guides(colour="none")
```

## Example: US consumption expenditure
\scriptsize
```{r ScatterMatrix, tidy=TRUE,fig.height=4}
uschange %>% as.data.frame %>% GGally::ggpairs(columnLabels = c("Cons", "Income", "Prod", "Savings", "Unemp"))
```

## Example: US consumption expenditure{.plain}

\fontsize{7.4}{6}

```{r usestim, tidy=TRUE, tidy.opts=c(width.cutoff = 70)}
fit.consMR <- tslm(Consumption ~ Income + Production + Unemployment + Savings, data=uschange)
summary(fit.consMR)
```


## Example: US consumption expenditure
\scriptsize
```{r usfitted1, tidy=TRUE, tidy.opts=c(width.cutoff = 40)}
autoplot(uschange[,'Consumption'], series="Data") + autolayer(fitted(fit.consMR), series="Fitted") +
labs(x = "Year", y = "") + ggtitle("% change in US consumption expenditure") + guides(colour=guide_legend(title=" "))
```

## Goodness of Fit: $R^2$
\scriptsize
A common way to summarise how well a linear regression model fits the data is via the coefficient of determination, or $R^2$. $R^2$ tells us how much of the variation in our dependent variable (`y`) is explained by the regressors (`x`s). We can calculate this as
$$R^2 = \frac{\sum\limits_{t=1}^T(\hat{y}_t - \bar{y})^2}{\sum\limits_{t=1}^T(y_t - \bar{y})^2} = \frac{\mbox{Explained (Regression) Sum of Squares}}{\mbox{Total Sum of Squares}}$$
\underline{Assuming that the model has an intercept:}

- If the predictions are close to the actual values, we would expect $R^2$ to be close to 1. 

- If the predictions are unrelated to the actual values, then $R^2 = 0$

**In all cases, $0\le R^2\le 1$. **   

We must be careful when comparing models on the basis of $R^2$ 
  : The value of $R^2$ will **never decrease** when adding an extra predictor to the model and this can lead to over-fitting.
  
  : We could be dealing with a "spurious" regression. We will get back to this at a point later.


## Example: US consumption expenditure
\scriptsize
```{r usfitted2, message=FALSE, tidy = TRUE, tidy.opts=c(width.cutoff = 70)}
cbind(Data=uschange[,"Consumption"], Fitted=fitted(fit.consMR)) %>% as.data.frame %>% ggplot(aes(x=Data, y=Fitted)) + geom_point() + xlab("Fitted (predicted values)") + ylab("Data (actual values)") + ggtitle("% change in US consumption expenditure") + geom_abline(intercept=0, slope=1)
```

## Example: US consumption expenditure

```{r}
checkresiduals(fit.consMR, test=FALSE)
```

# Residual diagnostics

##  Multiple regression and forecasting

1. We assume that the model is a reasonable approximation to reality; that is, the relationship between the forecast variable and the predictor variables satisfies this linear equation.

2. We make the following assumptions about the errors ($\varepsilon_1$, \dots, $\varepsilon_T$):

* $\varepsilon_t$ are uncorrelated and zero mean: otherwise the forecasts will be systematically biased.

* $\varepsilon_t$ are uncorrelated with each $x_{j,t}$: otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited.

* They are unrelated to the predictor variables: otherwise there would be more information that should be included in the systematic part of the model.

It is also useful to have $\varepsilon_t \sim \text{N}(0,\sigma^2)$ when producing prediction intervals or doing statistical tests.

## Residual plots

Useful for spotting outliers and whether the linear model was
appropriate.

* Scatterplot of residuals $\varepsilon_t$ against each predictor $x_{j,t}$.

* Scatterplot residuals against the fitted values $\hat y_t$

* Expect to see scatterplots resembling a horizontal band with
no values too far from the band and no patterns such as curvature or
increasing spread.

## Residual patterns

* If a plot of the residuals vs any predictor **in** the model shows a pattern, then the relationship is nonlinear.

* If a plot of the residuals vs any predictor **not** in the model shows a pattern, then the predictor should be added to the model.

* If a plot of the residuals vs fitted values shows a pattern, then there is *heteroscedasticity* in the errors. (Could try a transformation.)


## Residual patterns{.plain}
```{r, warning=FALSE, message=FALSE, echo = FALSE, fig.height=5}
g1 <- qplot(y = residuals(fit.consMR),x = uschange[,"Income"]) + labs(x = "Income", y = "Residuals")
g2 <- qplot(y = residuals(fit.consMR),x = uschange[,"Production"]) + labs(x = "Production", y = "Residuals")
g3 <- qplot(y = residuals(fit.consMR),x = uschange[,"Unemployment"]) + labs(x = "Unemploymnet", y = "Residuals")
g4 <- qplot(y = residuals(fit.consMR),x = uschange[,"Savings"]) + labs(x = "Savings", y = "Residuals")

gridExtra::grid.arrange(g1,g2,g3,g4, nrow= 2)
```

## Breusch-Godfrey test

**OLS regression:**
$$
y_{t}=\beta_{0}+\beta_{1}x_{t,1}+\dots+\beta_{k}x_{t,k}+u_{t}
$$
**Auxiliary regression:**
$$
{\hat {u}}_{t}=\beta_{0}+\beta_{1}x_{t,1}+\dots+\beta_{k}x_{t,k}+\rho_{1}{\hat {u}}_{t-1}+\dots +\rho_{p}{\hat {u}}_{t-p}+\varepsilon_{t}
$$

###
If $R^{2}$ statistic is calculated for the auxilliary model, then
$$
  (T-p)R^{2}\,\sim \,\chi_{p}^{2},
$$
Here, we are testing that there is no serial correlation up to lag $p$. $T=$ length of series.


* Breusch-Godfrey test better than Ljung-Box for regression models.

## US consumption again

\fontsize{9}{13}
```{r}
checkresiduals(fit.consMR, plot=FALSE)
```

### If the model fails the Breusch-Godfrey test \dots

* The forecasts are not wrong, but have higher variance than they need to.
* There is information in the residuals that we should exploit.
* This is done with a regression model with ARMA errors.
$H_0:$ There is no autocorrelation up to lag $p$.

# Some useful predictors for linear models

## Trend

**Linear trend**

\fontsize{13}{15}\selectfont
\begin{block}{Given the general form:}\vspace*{-0.3cm}
$$
  y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t} + \cdots + \beta_kx_{k,t} + \varepsilon_t.
$$
\end{block}
We can introduce a trend to the model by including $x_t = t$ as a regressor,
$$ y_t = \beta_0 + \beta_1 t + \varepsilon $$
where  $t=1,2,\dots,T$ 

A trend variable can be specified in the `tslm()` function using the `trend` predictor. 

Why do you want to include a trend?

  : Strong assumption that trend will continue.

## Dummy variables
\scriptsize
\begin{block}{}
If a categorical variable takes only two values (e.g., `Yes' or `No'), then an equivalent numerical variable can be constructed taking value 1 if yes and 0 if no. This is called a \textbf{dummy variable}.
\end{block}

\structure{Example}

Suppose we have quarterly retail sales data and suspect that there might be seasonality in our data (e.g. Q4 might have unusually high sales figures since we have Thanksgiving, Black Friday, Cyber Monday, and Christmas in Nov. & Dec.)

\tiny
|         | $Q_{1,t}$ | $Q_{2,t}$ | $Q_{3,t}$ |
|---------|:---------:|:---------:|:---------:|
| 2000 Q1 |     1     |     0     |     0     |
| 2000 Q2 |     0     |     1     |     0     |
| 2000 Q3 |     0     |     0     |     1     |
| 2000 Q4 |     0     |     0     |     0     |
| 2001 Q1 |     1     |     0     |     0     |
| 2001 Q2 |     0     |     1     |     0     |
| 2001 Q3 |     0     |     0     |     1     |
| 2001 Q4 |     0     |     0     |     0     |
| 2002 Q1 |     1     |     0     |     0     |
| 2002 Q2 |     0     |     1     |     0     |
| 2002 Q3 |     0     |     0     |     1     |
| 2002 Q4 |     0     |     0     |     0     |
| \vdots  |   \vdots  |   \vdots  |   \vdots  |


## Dummy variables

\begin{block}{Beware of the dummy variable trap! }
If there are more than two categories, then the variable can be coded using several dummy variables (one fewer than the total number of categories).
\end{block}

* Using one dummy for each category gives too many dummy variables!


* The regression will then be singular and inestimable.


* Either omit the constant, or omit the dummy for one category.


* If we omit one category, the coefficients of the remaining dummies are relative to that omitted category.

## Uses of dummy variables

\fontsize{13}{15}

**Seasonal dummies**

* For quarterly data: use 3 dummies
* For monthly data: use 11 dummies
* For daily data: use 6 dummies
* What to do with weekly data?

**Outliers**

* If there is an outlier, you can use a dummy variable (taking value 1 for that observation and 0 elsewhere) to remove its effect.

**Public holidays**

* For daily data: if it is a public holiday, dummy=1, otherwise dummy=0.

## Beer production revisited
\scriptsize
```{r, tidy=TRUE, tidy.opts=c(width.cutoff= 50)}
beer2 <- window(ausbeer, start=1992)
autoplot(beer2) + xlab("Year") + ylab("megalitres") + ggtitle("Australian quarterly beer production")
```

## Beer production revisited {.plain}
\scriptsize
\begin{block}{Regression model}
$$y_t = \beta_0 + \beta_1 t + \beta_2d_{2,t} + \beta_3 d_{3,t} + \beta_4 d_{4,t} + \varepsilon_t$$
\end{block}

* $d_{i,t} = 1$ if $t$ is quarter $i$ and 0 otherwise , for $i \in [2,4]$.


```{r, seasbeer, echo = FALSE, results = "hold", fig.height=3.5, tidy = TRUE, tidy.opts=c(width.cutoff= 50)}
g1 <- ggseasonplot(beer2,year.labels = TRUE) + ggtitle("Season Plot: Aus. beer production")
g2 <- ggsubseriesplot(beer2) + ggtitle("SubSeries Plot: Aus. beer production") + ylab(" ")
gridExtra::grid.arrange(g1,g2, nrow = 1)
```


## Beer production revisited
\fontsize{8}{6.5} 
```{r}
fit.beer <- tslm(beer2 ~ trend + season) # Model with trend and seasonal dummies
summary(fit.beer)
```
## Beer production revisited
\scriptsize
```{r, tidy = TRUE, tidy.opts=c(width.cutoff= 50)}
autoplot(beer2, series="Data") +  autolayer(fitted(fit.beer), series="Fitted") + xlab("Year") +
  ylab("Megalitres") + ggtitle("Quarterly Beer Production")
```

## Beer production revisited
\scriptsize
```{r, tidy= TRUE, tidy.opts=c(width.cutoff= 50)}
cbind(Data=beer2, Fitted=fitted(fit.beer)) %>%  as.data.frame %>% ggplot(aes(x=Data, y=Fitted, colour=as.factor(cycle(beer2)))) + geom_point() + labs(y = "Fitted", x = "Actual values") +
    ggtitle("Quarterly beer production") + scale_colour_brewer(palette="Dark2", name="Quarter") +
    geom_abline(intercept=0, slope=1)
```

## Beer production revisited

```{r}
checkresiduals(fit.beer, test=FALSE)
```

## Forecasting Beer production

```{r}
fit.beer %>% forecast %>% autoplot
```

## Fourier series

Periodic seasonality can be handled using pairs of Fourier terms:
$$ s_{k}(t) = \sin\left(\frac{2\pi k t}{m}\right)\qquad c_{k}(t) = \cos\left(\frac{2\pi k t}{m}\right)$$
$$ y_t = a + bt + \sum_{k=1}^K \bigg[\alpha_k s_k(t) + \beta_k c_k(t)\bigg] + \varepsilon_t$$

* where `m` is the seasonal period.
* Every periodic function can be approximated by sums of sin and cos terms for large enough $K$.
* Choose $K$ by minimizing AICc.
* Called "harmonic regression"

```r
fit <- tslm(y ~ trend + fourier(y, K))
```

## Harmonic regression: beer production

\fontsize{7}{6.5}

```{r fourierbeer}
fourier.beer <- tslm(beer2 ~ trend + fourier(beer2, K=2)) #maximum allowed is K = m/2
summary(fourier.beer)
```

## Intervention variables

**Spikes**
  :  - the effect of an event lasts for only 1 period. We use a dummy variable to capture this.

  : - equivalent to a dummy variable for handling an outlier.

**Steps**

  : - the intervention has an immediate and permanent effect. The intervention causes a level shift.
  
  : - dummy variable takes value 0 before the intervention and 1 afterwards.

**Change of slope**

  : - the intervention causes a permanent effect and changes the slope. 
  
  : - we use a piecewise trend here. The trend is no longer linear.

  : - variables take values 0 before the intervention and values $\{1,2,3,\dots\}$ afterwards.


## Distributed lags

Lagged values of a predictor.

Example: $x$ is advertising which has a delayed effect

\begin{align*}
  x_{1} &= \text{advertising for previous month;} \\
  x_{2} &= \text{advertising for two months previously;} \\
        & \vdots \\
  x_{m} &= \text{advertising for $m$ months previously.}
\end{align*}


## Nonlinear trend

**Piecewise linear trend with bend at $\tau$**
\begin{align*}
x_{1,t} &= t \\
x_{2,t} &= \left\{ \begin{array}{ll}
  0 & t <\tau\\
  (t-\tau) & t \ge \tau
\end{array}\right.
\end{align*}

*It is perfectly fine if you do not completely get this at this point. The mathematical notation can be a bit intimidating.*

**Quadratic or higher order trend**
\[  x_{1,t} =t,\quad x_{2,t}=t^2,\quad \dots \]

\center{\textcolor{orange}{\textbf{NOT RECOMMENDED!}: to use quadratic or higher order trends in forecasting. When they are extrapolated, the resulting forecasts are often unrealistic.}}

# Selecting predictors and forecast evaluation

## Selecting predictors

* When there are many predictors, how should we choose which ones to use?

* We need a way of comparing two competing models.

**What not to do!**

* Plot $y$ against a particular predictor ($x_j$) and if it shows no noticeable relationship, drop it.

* Do a multiple linear regression on all the predictors and disregard all variables whose  $p$ values are greater than 0.05.

* Maximize $R^2$ or minimize MSE

## Comparing regression models
\scriptsize
Computer output for regression will always give the $R^2$ value. This is a
useful summary of the model.

However \dots

* $R^2$  does not allow for ``degrees of freedom''.

* Adding *any* variable tends to increase the value of $R^2$, even if that variable is irrelevant.

To overcome this problem, we can use \emph{adjusted $R^2$}:

\begin{block}{}
\[\bar{R}^2 = 1-(1-R^2)\frac{T-1}{T-k-1}\]
where $k=$ no.\ predictors and $T=$ no.\ observations.
\end{block}

\begin{alertblock}{Maximizing $\bar{R}^2$ is equivalent to minimizing $\hat\sigma^2$.}
\centerline{$\displaystyle
\hat{\sigma}^2 = \frac{1}{T-k-1}\sum_{t=1}^T \varepsilon_t^2$
}
\end{alertblock}

## Akaike's Information Criterion

\begin{block}{}
\[\mbox{AIC} = -2\log(L) + 2(k+2)\]
\end{block}
where $L$ is the likelihood and $k$ is the number of predictors in the model.

* This is a \emph{penalized likelihood} approach.
* *Minimizing* the AIC gives the best model for prediction.
* AIC penalizes terms more heavily than $\bar{R}^2$.
* Minimizing the AIC is asymptotically equivalent to minimizing MSE via leave-one-out cross-validation.

## Corrected AIC

For small values of $T$, the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed.

\begin{block}{}
$$ \text{AIC}_{\text{C}} = \text{AIC} + \frac{2(k+2)(k+3)}{T-k-3} $$
\end{block}

As with the AIC, the AIC$_{\text{C}}$ should be minimized.

## Bayesian Information Criterion
\begin{block}{}
$$\mbox{BIC} = -2\log(L) + (k+2)\log(T) $$
\end{block}

where $L$ is the likelihood and $k$ is the number of predictors in the model.

* BIC penalizes terms more heavily than AIC

* Also called SBIC and SC.

* Minimizing BIC is asymptotically equivalent to leave-$v$-out cross-validation when $v = T\bigg[1-\frac{1}{(\log(T)-1)}\bigg]$.

## Choosing regression variables

**Best subsets regression**

* Fit all possible regression models using one or more of the predictors.

* Choose the best model based on one of the measures of predictive ability (CV, AIC, AICc).

**Warning!**

* If there are a large number of predictors, this is not possible.

* For example, 44 predictors leads to 18 trillion possible models!

## Choosing regression variables

**Backwards stepwise regression**

* Start with a model containing all variables.

* Try subtracting one variable at a time. Keep the model if it
has lower CV or AICc.

* Iterate until no further improvement.

**Notes**

* Stepwise regression is not guaranteed to lead to the best possible
model.

* Inference on coefficients of  final model will be wrong.

## Cross-validation
\scriptsize\sf
```{r, tidy = TRUE, tidy.opts=c(width.cutoff = 0.8), results="hold"}
tslm(Consumption ~ Income + Production + Unemployment + Savings, data=uschange) %>% CV() %>% round(3)
tslm(Consumption ~ Income + Production + Unemployment, data=uschange) %>% CV() %>% round(3)
tslm(Consumption ~ Income + Production + Savings, data=uschange) %>% CV() %>% round(3)
tslm(Consumption ~ Income + Unemployment + Savings, data=uschange) %>% CV() %>% round(3)
tslm(Consumption ~ Production + Unemployment + Savings, data=uschange) %>% CV() %>% round(3)
```

# Forecasting with regression

## Ex-ante versus ex-post forecasts

 * *Ex ante forecasts* are made using only information available in advance.
 
    - require forecasts of predictors
    
 * *Ex post forecasts* are made using later information on the predictors.
 
    - useful for studying behaviour of forecasting models.
    
 * trend, seasonal and calendar variables are all known in advance, so these don't need to be forecasted.
 
## Scenario based forecasting

 * Assumes possible scenarios for the predictor variables
 
 * Prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables.
 
## Building a predictive regression model

 * If getting forecasts of predictors is difficult, you can use lagged predictors instead.
$$y_{t}=\beta_0+\beta_1x_{1,t-h}+\dots+\beta_kx_{k,t-h}+\varepsilon_{t}$$

 * A different model for each forecast horizon $h$.


## Beer production
\scriptsize
```{r beeryetagain, fig.height=3.5, tidy = TRUE, tidy.opts=c(width.cutoff = 70)}
beer2 <- window(ausbeer, start=1992)
fit.beer <- tslm(beer2 ~ trend + season)
fcast <- forecast(fit.beer)
autoplot(fcast) + ggtitle("Forecasts of beer production using regression") + labs(x = "Year", y = "megalitres")
```

## US Consumption

\footnotesize
  \structure{Example}
 A US policy maker is interested in comparing the predicted change in consumption when there is a constant growth of 1% and 0.5% respectively for income and savings with no change in the employment rate, versus a respective decline of 1% and 0.5%, for each of the four quarters following the end of the sample. 
 
Note that prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables. They assume that the values of the predictors are known in advance.


```{r usconsumptionf, tidy = TRUE, tidy.opts=c(width.cutoff = 60)}
fit.consBest <- tslm(Consumption ~ Income + Savings + Unemployment, data = uschange)
h <- 4
newdata <- data.frame(Income = c(1, 1, 1, 1),
    Savings = c(0.5, 0.5, 0.5, 0.5),
    Unemployment = c(0, 0, 0, 0))
fcast.up <- forecast(fit.consBest, newdata = newdata)
newdata <- data.frame(Income = rep(-1, h),
    Savings = rep(-0.5, h),
    Unemployment = rep(0, h))
fcast.down <- forecast(fit.consBest, newdata = newdata)
```

## US Consumption
\fontsize{10}{10}\sf

```{r usconsumptionf2, fig.height=3.5}
autoplot(uschange[, 1]) +
  ylab("% change in US consumption") +
  autolayer(fcast.up, PI = TRUE, series = "increase") +
  autolayer(fcast.down, PI = TRUE, series = "decrease") +
  guides(colour = guide_legend(title = "Scenario"))
```

# Correlation, causation and forecasting

## Correlation is not causation

* When $x$ is useful for predicting $y$, it is not necessarily causing $y$.

* e.g., predict number of drownings $y$ using number of ice-creams sold $x$.

* Correlations are useful for forecasting, even when there is no causality.

* Better models usually involve causal relationships (e.g., temperature $x$ and people $z$ to predict drownings $y$).

## Multicollinearity
In regression analysis, multicollinearity occurs when:

*  Two  predictors are highly  correlated (i.e., the correlation between them is close to $\pm1$).

* A linear combination of some of the predictors is highly correlated  with another predictor.

*  A linear combination of one subset of predictors is highly correlated with a linear combination of another
  subset of predictors.

## Multicollinearity

If multicollinearity exists \dots

* the numerical estimates of coefficients may be wrong (worse in Excel than in a statistics package)
* don't rely on the $p$-values to determine significance.
* there is no problem with model *predictions* provided the predictors used for forecasting are within the range used for fitting.
* omitting variables can help.
* combining variables can help.

## Outliers and influential observations

**Things to watch for**

1. *Outliers*
  : observations that produce large residuals.
  
2. *Influential observations*
  : removing them would markedly change the coefficients.  (Often outliers in the $x$ variable).
  
3. *Lurking variable*
  : a predictor not included in the regression but which has an important effect on the response.
  
4. Points should not normally be removed without a good explanation of why they are different.
